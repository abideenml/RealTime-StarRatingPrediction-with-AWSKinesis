{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Analytics and Machine Learning over Streaming Data\n",
    "\n",
    "Streaming technologies provide you with the tools to collect, process, and analyze data streams in real time. AWS offers a wide range of streaming technology options including Amazon Managed Streaming for Apache Kafka (Amazon MSK), and the [Amazon Kinesis](https://aws.amazon.com/kinesis/) family of services. \n",
    "\n",
    "With Kinesis Data Firehose, you can prepare and load the data continuously to a destination of your choice. With Kinesis Data Analytics, you can process and analyze the data as it arrives. And with Kinesis Data Streams, you c an manage the ingest of data streams for custom applications. \n",
    "\n",
    "In this section, we move from our customer reviews training dataset into a real-world scenario. Customer feedback about products appear in all of a company's social media channels, on partner websites, in customer support messages etc. We need to capture this valuable customer sentiment about our products as quickly as possible to spot trends and react fast.\n",
    "\n",
    "We will focus on analyzing a continuous stream of product review messages that we collect from all available online channels. \n",
    "\n",
    "![](data/readme_pics/streaming-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we analyze the sentiment of the customer, so we can identify which customers  might need high-priority attention. \n",
    "\n",
    "Next, we run continuous streaming analytics over the incoming review messages to capture the average sentiment per product category. We visualize the continuous average sentiment in a metrics dashboard for the line of business owners. The line of business owners can now detect sentiment trends quickly,  and take action. \n",
    "\n",
    "We also calculate an anomaly score of the incoming messages to detect anomalies in the data schema or data values. In case of a rising anomaly score, we can alert the application developers in charge to investigate the root cause. \n",
    "\n",
    "As a last metric, we also calculate a continuous approximate count of the received messages. This number of online messages could be used by the digital marketing team to measure effectiveness of social media campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Kinesis Data Firehose vs. Kinesis Data Streams_\n",
    "\n",
    "### Kinesis Data Firehose\n",
    "* Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. \n",
    "* It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards youâ€™re already using today. \n",
    "* It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\n",
    "\n",
    "### Kinesis Data Streams\n",
    "* Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. \n",
    "* You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources. \n",
    "* Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup IAM for Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name \n",
    "\n",
    "sts = boto3.Session().client(service_name=\"sts\", region_name=region)\n",
    "iam = boto3.Session().client(service_name=\"iam\", region_name=region)\n",
    "\n",
    "# Create Kinesis Role\n",
    "iam_kinesis_role_name = \"DSOAWS_Kinesis\"\n",
    "iam_kinesis_role_passed = False\n",
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"kinesis.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"},\n",
    "        {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"firehose.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"},\n",
    "        {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"kinesisanalytics.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"},\n",
    "    ],\n",
    "}\n",
    "import json\n",
    "import time\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    iam_role_kinesis = iam.create_role(\n",
    "        RoleName=iam_kinesis_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "        Description=\"DSOAWS Kinesis Role\",\n",
    "    )\n",
    "    print(\"Role succesfully created.\")\n",
    "    iam_kinesis_role_passed = True\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        iam_role_kinesis = iam.get_role(RoleName=iam_kinesis_role_name)\n",
    "        print(\"Role already exists. That is OK.\")\n",
    "        iam_kinesis_role_passed = True\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "iam_role_kinesis_name = iam_role_kinesis[\"Role\"][\"RoleName\"]\n",
    "print(\"Role Name: {}\".format(iam_role_kinesis_name))\n",
    "iam_role_kinesis_arn = iam_role_kinesis[\"Role\"][\"Arn\"]\n",
    "print(\"Role ARN: {}\".format(iam_role_kinesis_arn))\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "# Stream Name\n",
    "stream_name = \"dsoaws-kinesis-data-stream\"\n",
    "# Specify Firehose Name\n",
    "firehose_name = \"dsoaws-kinesis-data-firehose\"\n",
    "# Specify Lambda Function Name\n",
    "lambda_fn_name_cloudwatch = \"DeliverKinesisAnalyticsToCloudWatch\"\n",
    "lambda_fn_name_invoke_sm_endpoint = \"InvokeSageMakerEndpointFromKinesis\"\n",
    "lambda_fn_name_sns = \"PushNotificationToSNS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Update Policy\n",
    "\n",
    "kinesis_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:ListBucketMultipartUploads\",\n",
    "                \"s3:PutObject\",\n",
    "            ],\n",
    "            \"Resource\": [\"arn:aws:s3:::{}\".format(bucket), \"arn:aws:s3:::{}/*\".format(bucket)],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"logs:PutLogEvents\"],\n",
    "            \"Resource\": [\"arn:aws:logs:{}:{}:log-group:/*\".format(region, account_id)],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kinesis:*\",\n",
    "            ],\n",
    "            \"Resource\": [\"arn:aws:kinesis:{}:{}:stream/{}\".format(region, account_id, stream_name)],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"firehose:*\",\n",
    "            ],\n",
    "            \"Resource\": [\"arn:aws:firehose:{}:{}:deliverystream/{}\".format(region, account_id, firehose_name)],\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kinesisanalytics:*\",\n",
    "            ],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"UseLambdaFunction\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"lambda:InvokeFunction\", \"lambda:GetFunctionConfiguration\"],\n",
    "            \"Resource\": [\"*\"],\n",
    "        },\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"iam:PassRole\", \"Resource\": [\"arn:aws:iam::*:role/service-role/kinesis*\"]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(json.dumps(kinesis_policy_doc, indent=4, sort_keys=True, default=str))\n",
    "response = iam.put_role_policy(\n",
    "    RoleName=iam_role_kinesis_name, PolicyName=\"DSOAWS_KinesisPolicy\", PolicyDocument=json.dumps(kinesis_policy_doc)\n",
    ")\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "print(json.dumps(response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AWS Lambda IAM Role\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "iam_lambda_role_name = \"DSOAWS_Lambda\"\n",
    "iam_lambda_role_passed = False\n",
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"},\n",
    "        {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"kinesisanalytics.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    iam_role_lambda = iam.create_role(\n",
    "        RoleName=iam_lambda_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "        Description=\"DSOAWS Lambda Role\",\n",
    "    )\n",
    "    print(\"Role succesfully created.\")\n",
    "    iam_lambda_role_passed = True\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        iam_role_lambda = iam.get_role(RoleName=iam_lambda_role_name)\n",
    "        print(\"Role already exists. This is OK.\")\n",
    "        iam_lambda_role_passed = True\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "iam_role_lambda_name = iam_role_lambda[\"Role\"][\"RoleName\"]\n",
    "print(\"Role Name: {}\".format(iam_role_lambda_name))\n",
    "iam_role_lambda_arn = iam_role_lambda[\"Role\"][\"Arn\"]\n",
    "print(\"Role ARN: {}\".format(iam_role_lambda_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AWS Lambda IAM Policy\n",
    "\n",
    "lambda_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"UseLambdaFunction\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"lambda:InvokeFunction\", \"lambda:GetFunctionConfiguration\"],\n",
    "            \"Resource\": \"arn:aws:lambda:{}:{}:function:*\".format(region, account_id),\n",
    "        },\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"cloudwatch:*\", \"Resource\": \"*\"},\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"sns:*\", \"Resource\": \"*\"},\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"arn:aws:logs:{}:{}:*\".format(region, account_id),\n",
    "        },\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"sagemaker:InvokeEndpoint\", \"Resource\": \"*\"},\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"logs:CreateLogStream\", \"logs:PutLogEvents\"],\n",
    "            \"Resource\": \"arn:aws:logs:{}:{}:log-group:/aws/lambda/*\".format(region, account_id),\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(json.dumps(lambda_policy_doc, indent=4, sort_keys=True, default=str))\n",
    "response = iam.put_role_policy(\n",
    "    RoleName=iam_role_lambda_name, PolicyName=\"DSOAWS_LambdaPolicy\", PolicyDocument=json.dumps(lambda_policy_doc)\n",
    ")\n",
    "\n",
    "time.sleep(30)\n",
    "print(json.dumps(response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lambda To Invoke Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review Lambda Function\n",
    "\n",
    "lambda_fn_name_invoke_ep = \"InvokeSageMakerEndpointFromKinesis\"\n",
    "!pygmentize src/invoke_sm_endpoint_from_kinesis.py\n",
    "\n",
    "# Test the PyTorch Endpoint Similar to How the Lambda Invokes the Endpoint\n",
    "inputs = [\n",
    "    {\"features\": [\"I love this product!\"]},\n",
    "    {\"features\": [\"OK, but not great.\"]},\n",
    "    {\"features\": [\"This is not the right product.\"]},\n",
    "]\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=pytorch_endpoint_name,\n",
    "    serializer=JSONLinesSerializer(),\n",
    "    deserializer=JSONLinesDeserializer(),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    print(\"Predicted class: {}\".format(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .zip file for the Python dependencies (Lambda Layer)\n",
    "# This requires us to create a directory called `python` for Python environments.\n",
    "\n",
    "!rm -rf layer/python\n",
    "!mkdir -p layer/python\n",
    "!pip install -q --target layer/python sagemaker==2.38.0\n",
    "!cd layer && zip -q --recurse-paths layer.zip .\n",
    "\n",
    "with open(\"layer/layer.zip\", \"rb\") as f:\n",
    "    layer = f.read()\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sagemaker_lambda_layer_name = 'sagemaker-python-sdk-layer'\n",
    "layer_response = iam.publish_layer_version(\n",
    "    LayerName=sagemaker_lambda_layer_name,\n",
    "    Content={\"ZipFile\": layer},\n",
    "    Description=\"Layer with 'pip install sagemaker'\",\n",
    "    CompatibleRuntimes=['python3.9']\n",
    ")\n",
    "\n",
    "layer_version_arn = layer_response['LayerVersionArn']\n",
    "print(\"Lambda layer {} successfully created with LayerVersionArn {}.\".format(sagemaker_lambda_layer_name, layer_version_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .zip file for Python code\n",
    "!zip src/InvokeSageMakerEndpointFromKinesis.zip src/invoke_sm_endpoint_from_kinesis.py\n",
    "with open(\"src/InvokeSageMakerEndpointFromKinesis.zip\", \"rb\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "# Create The Lambda Function\n",
    "\n",
    "try:\n",
    "    response = iam.create_function(\n",
    "        FunctionName=\"{}\".format(lambda_fn_name_invoke_ep),\n",
    "        Runtime=\"python3.9\",\n",
    "        Role=\"{}\".format(iam_role_lambda_arn),\n",
    "        Handler=\"src/invoke_sm_endpoint_from_kinesis.lambda_handler\",\n",
    "        Code={\"ZipFile\": code},\n",
    "        Layers=[\n",
    "            layer_version_arn\n",
    "        ],\n",
    "        Description=\"Query SageMaker Endpoint for star rating prediction on review input text.\",\n",
    "        # max timeout supported by Firehose is 5min\n",
    "        Timeout=300,\n",
    "        MemorySize=128,\n",
    "        Publish=True,\n",
    "    )\n",
    "    print(\"Lambda Function {} successfully created.\".format(lambda_fn_name_invoke_ep))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceConflictException\":\n",
    "        response = iam.update_function_code(\n",
    "            FunctionName=\"{}\".format(lambda_fn_name_invoke_ep), ZipFile=code, Publish=True, DryRun=False\n",
    "        )\n",
    "        print(\"Updating existing Lambda Function {}.  This is OK.\".format(lambda_fn_name_invoke_ep))\n",
    "    else:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "response = iam.get_function(FunctionName=lambda_fn_name_invoke_ep)\n",
    "\n",
    "lambda_fn_arn_invoke_ep = response[\"Configuration\"][\"FunctionArn\"]\n",
    "print(lambda_fn_arn_invoke_ep)\n",
    "\n",
    "response = iam.update_function_configuration(\n",
    "    FunctionName=lambda_fn_name_invoke_ep, Environment={\"Variables\": {\"ENDPOINT_NAME\": pytorch_endpoint_name}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kinesis Data Firehose Delivery Stream\n",
    "\n",
    "<img src=\"../readme_pics/realtime-streaming.png\" width=\"100%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firehose = boto3.Session().client(service_name=\"firehose\", region_name=region)\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    response = firehose.create_delivery_stream(\n",
    "        DeliveryStreamName=firehose_name,\n",
    "        DeliveryStreamType=\"DirectPut\",\n",
    "        ExtendedS3DestinationConfiguration={\n",
    "            \"RoleARN\": iam_role_kinesis_arn,\n",
    "            \"BucketARN\": \"arn:aws:s3:::{}\".format(bucket),\n",
    "            \"Prefix\": \"kinesis-data-firehose/\",\n",
    "            \"ErrorOutputPrefix\": \"kinesis-data-firehose-error/\",\n",
    "            \"BufferingHints\": {\"SizeInMBs\": 1, \"IntervalInSeconds\": 60}, \n",
    "            \"CompressionFormat\": \"UNCOMPRESSED\",\n",
    "            \"CloudWatchLoggingOptions\": {\n",
    "                \"Enabled\": True,\n",
    "                \"LogGroupName\": \"/aws/kinesisfirehose/dsoaws-kinesis-data-firehose\",\n",
    "                \"LogStreamName\": \"S3Delivery\",\n",
    "            },\n",
    "            \"ProcessingConfiguration\": {\n",
    "                \"Enabled\": True,\n",
    "                \"Processors\": [\n",
    "                    {\n",
    "                        \"Type\": \"Lambda\",\n",
    "                        \"Parameters\": [\n",
    "                            {\n",
    "                                \"ParameterName\": \"LambdaArn\",\n",
    "                                \"ParameterValue\": \"{}:$LATEST\".format(lambda_fn_arn_invoke_ep),\n",
    "                            },\n",
    "                            {\"ParameterName\": \"BufferSizeInMBs\", \"ParameterValue\": \"1\"},\n",
    "                            {\"ParameterName\": \"BufferIntervalInSeconds\", \"ParameterValue\": \"60\"},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            \"S3BackupMode\": \"Enabled\",\n",
    "            \"S3BackupConfiguration\": {\n",
    "                \"RoleARN\": iam_role_kinesis_arn,\n",
    "                \"BucketARN\": \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"Prefix\": \"kinesis-data-firehose-source-record/\",\n",
    "                \"ErrorOutputPrefix\": \"!{firehose:error-output-type}/\",\n",
    "                \"BufferingHints\": {\"SizeInMBs\": 1, \"IntervalInSeconds\": 60},\n",
    "                \"CompressionFormat\": \"UNCOMPRESSED\",\n",
    "            },\n",
    "            \"CloudWatchLoggingOptions\": {\n",
    "                \"Enabled\": False,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    print(\"Delivery stream {} successfully created.\".format(firehose_name))\n",
    "    print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceInUseException\":\n",
    "        print(\"Delivery stream {} already exists.\".format(firehose_name))\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"\"\n",
    "while status != \"ACTIVE\":\n",
    "    r = firehose.describe_delivery_stream(DeliveryStreamName=firehose_name)\n",
    "    description = r.get(\"DeliveryStreamDescription\")\n",
    "    status = description.get(\"DeliveryStreamStatus\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"Delivery Stream {} is active\".format(firehose_name))\n",
    "firehose_arn = r[\"DeliveryStreamDescription\"][\"DeliveryStreamARN\"]\n",
    "print(firehose_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kinesis Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis = boto3.Session().client(service_name=\"kinesis\", region_name=region)\n",
    "shard_count = 2\n",
    "try:\n",
    "    response = kinesis.create_stream(StreamName=stream_name, ShardCount=shard_count)\n",
    "    print(\"Data Stream {} successfully created.\".format(stream_name))\n",
    "    print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceInUseException\":\n",
    "        print(\"Data Stream {} already exists.\".format(stream_name))\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "status = \"\"\n",
    "while status != \"ACTIVE\":\n",
    "    r = kinesis.describe_stream(StreamName=stream_name)\n",
    "    description = r.get(\"StreamDescription\")\n",
    "    status = description.get(\"StreamStatus\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"Stream {} is active\".format(stream_name))\n",
    "stream_response = kinesis.describe_stream(StreamName=stream_name)\n",
    "print(json.dumps(stream_response, indent=4, sort_keys=True, default=str))\n",
    "stream_arn = stream_response[\"StreamDescription\"][\"StreamARN\"]\n",
    "print(stream_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lambda Destination CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "lam = boto3.Session().client(service_name=\"lambda\", region_name=region)\n",
    "\n",
    "!pygmentize src/deliver_metrics_to_cloudwatch.py\n",
    "!zip src/DeliverKinesisAnalyticsToCloudWatch.zip src/deliver_metrics_to_cloudwatch.py\n",
    "with open(\"src/DeliverKinesisAnalyticsToCloudWatch.zip\", \"rb\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "# Create The Lambda Function\n",
    "try:\n",
    "    response = lam.create_function(\n",
    "        FunctionName=\"{}\".format(lambda_fn_name_cloudwatch),\n",
    "        Runtime=\"python3.9\",\n",
    "        Role=\"{}\".format(iam_role_lambda_arn),\n",
    "        Handler=\"src/deliver_metrics_to_cloudwatch.lambda_handler\",\n",
    "        Code={\"ZipFile\": code},\n",
    "        Description=\"Deliver output records from Kinesis Analytics application to CloudWatch.\",\n",
    "        Timeout=900,\n",
    "        MemorySize=128,\n",
    "        Publish=True,\n",
    "    )\n",
    "    print(\"Lambda Function {} successfully created.\".format(lambda_fn_name_cloudwatch))\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceConflictException\":\n",
    "        response = lam.update_function_code(\n",
    "            FunctionName=\"{}\".format(lambda_fn_name_cloudwatch), ZipFile=code, Publish=True, DryRun=False\n",
    "        )\n",
    "        print(\"Updating existing Lambda Function {}.  This is OK.\".format(lambda_fn_name_cloudwatch))\n",
    "    else:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "response = lam.get_function(FunctionName=lambda_fn_name_cloudwatch)\n",
    "\n",
    "lambda_fn_arn_cloudwatch = response[\"Configuration\"][\"FunctionArn\"]\n",
    "print(lambda_fn_arn_cloudwatch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lambda Destination SNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns = boto3.Session().client(service_name=\"sns\", region_name=region)\n",
    "topics = sns.list_topics()\n",
    "print(topics)\n",
    "response = sns.create_topic(\n",
    "    Name=\"review_anomaly_scores\",\n",
    ")\n",
    "print(response)\n",
    "sns_topic_arn = response[\"TopicArn\"]\n",
    "print(sns_topic_arn)\n",
    "\n",
    "!pygmentize src/push_notification_to_sns.py\n",
    "# Zip the Lambda Function\n",
    "!zip src/PushNotificationToSNS.zip src/push_notification_to_sns.py\n",
    "\n",
    "with open(\"src/PushNotificationToSNS.zip\", \"rb\") as f:\n",
    "    code = f.read()\n",
    "\n",
    "try:\n",
    "    response = lam.create_function(\n",
    "        FunctionName=\"{}\".format(lambda_fn_name_sns),\n",
    "        Runtime=\"python3.9\",\n",
    "        Role=\"{}\".format(iam_role_lambda_arn),\n",
    "        Handler=\"src/push_notification_to_sns.lambda_handler\",\n",
    "        Code={\"ZipFile\": code},\n",
    "        Description=\"Deliver output records from Kinesis Analytics application to CloudWatch.\",\n",
    "        Timeout=300,\n",
    "        MemorySize=128,\n",
    "        Publish=True,\n",
    "    )\n",
    "    print(\"Lambda Function {} successfully created.\".format(lambda_fn_name_sns))\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceConflictException\":\n",
    "        response = lam.update_function_code(\n",
    "            FunctionName=\"{}\".format(lambda_fn_name_sns), ZipFile=code, Publish=True, DryRun=False\n",
    "        )\n",
    "        print(\"Updating existing Lambda Function {}.  This is OK.\".format(lambda_fn_name_sns))\n",
    "    else:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "response = lam.get_function(FunctionName=lambda_fn_name_sns)\n",
    "\n",
    "lambda_fn_arn_sns = response[\"Configuration\"][\"FunctionArn\"]\n",
    "print(lambda_fn_arn_sns)\n",
    "\n",
    "# Update Lambda Function with SNS Topic ARN\n",
    "response = lam.update_function_configuration(\n",
    "    FunctionName=lambda_fn_name_sns, Environment={\"Variables\": {\"SNS_TOPIC_ARN\": sns_topic_arn}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Kinesis Data Analytics App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinesis_analytics = boto3.Session().client(service_name=\"kinesisanalytics\", region_name=region)\n",
    "\n",
    "# Kinesis Analytics Application Name\n",
    "kinesis_data_analytics_app_name = \"dsoaws-kinesis-data-analytics-sql-app\"\n",
    "in_app_stream_name = \"SOURCE_SQL_STREAM_001\"  # Default\n",
    "print(in_app_stream_name)\n",
    "\n",
    "## Create Application\n",
    "window_seconds = 5\n",
    "sql_code = \"\"\" \\\n",
    "        CREATE OR REPLACE STREAM \"AVG_STAR_RATING_SQL_STREAM\" ( \\\n",
    "            avg_star_rating DOUBLE); \\\n",
    "        CREATE OR REPLACE PUMP \"AVG_STAR_RATING_SQL_STREAM_PUMP\" AS \\\n",
    "            INSERT INTO \"AVG_STAR_RATING_SQL_STREAM\" \\\n",
    "                SELECT STREAM AVG(CAST(\"star_rating\" AS DOUBLE)) AS avg_star_rating \\\n",
    "                FROM \"{}\" \\\n",
    "                GROUP BY \\\n",
    "                STEP(\"{}\".ROWTIME BY INTERVAL '{}' SECOND); \\\n",
    "         \\\n",
    "        CREATE OR REPLACE STREAM \"ANOMALY_SCORE_SQL_STREAM\" (anomaly_score DOUBLE); \\\n",
    "        CREATE OR REPLACE PUMP \"ANOMALY_SCORE_STREAM_PUMP\" AS \\\n",
    "            INSERT INTO \"ANOMALY_SCORE_SQL_STREAM\" \\\n",
    "            SELECT STREAM anomaly_score \\\n",
    "            FROM TABLE(RANDOM_CUT_FOREST( \\\n",
    "                CURSOR(SELECT STREAM \"star_rating\" \\\n",
    "                    FROM \"{}\" \\\n",
    "            ) \\\n",
    "          ) \\\n",
    "        ); \\\n",
    "         \\\n",
    "        CREATE OR REPLACE STREAM \"APPROXIMATE_COUNT_SQL_STREAM\" (number_of_distinct_items BIGINT); \\\n",
    "        CREATE OR REPLACE PUMP \"APPROXIMATE_COUNT_STREAM_PUMP\" AS \\\n",
    "            INSERT INTO \"APPROXIMATE_COUNT_SQL_STREAM\" \\\n",
    "            SELECT STREAM number_of_distinct_items \\\n",
    "            FROM TABLE(COUNT_DISTINCT_ITEMS_TUMBLING( \\\n",
    "                CURSOR(SELECT STREAM \"review_id\" FROM \"{}\"), \\\n",
    "                'review_id', \\\n",
    "                {} \\\n",
    "              ) \\\n",
    "        ); \\\n",
    "    \"\"\".format(\n",
    "    in_app_stream_name, in_app_stream_name, window_seconds, in_app_stream_name, in_app_stream_name, window_seconds\n",
    ")\n",
    "\n",
    "print(sql_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = kinesis_analytics.create_application(\n",
    "        ApplicationName=kinesis_data_analytics_app_name,\n",
    "        Inputs=[\n",
    "            {\n",
    "                \"NamePrefix\": \"SOURCE_SQL_STREAM\",\n",
    "                \"KinesisFirehoseInput\": {\n",
    "                    \"ResourceARN\": \"{}\".format(firehose_arn),\n",
    "                    \"RoleARN\": \"{}\".format(iam_role_kinesis_arn),\n",
    "                },\n",
    "                \"InputProcessingConfiguration\": {\n",
    "                    \"InputLambdaProcessor\": {\n",
    "                        \"ResourceARN\": \"{}\".format(lambda_fn_arn_invoke_ep),\n",
    "                        \"RoleARN\": \"{}\".format(iam_role_lambda_arn),\n",
    "                    }\n",
    "                },\n",
    "                \"InputSchema\": {\n",
    "                    \"RecordFormat\": {\n",
    "                        \"RecordFormatType\": \"CSV\",\n",
    "                        \"MappingParameters\": {\n",
    "                            \"CSVMappingParameters\": {\"RecordRowDelimiter\": \"\\n\", \"RecordColumnDelimiter\": \"\\t\"}\n",
    "                        },\n",
    "                    },\n",
    "                    \"RecordColumns\": [\n",
    "                        {\"Name\": \"review_id\", \"Mapping\": \"review_id\", \"SqlType\": \"VARCHAR(14)\"},\n",
    "                        {\"Name\": \"star_rating\", \"Mapping\": \"star_rating\", \"SqlType\": \"INTEGER\"},\n",
    "                        {\"Name\": \"product_category\", \"Mapping\": \"product_category\", \"SqlType\": \"VARCHAR(24)\"},\n",
    "                        {\"Name\": \"review_body\", \"Mapping\": \"review_body\", \"SqlType\": \"VARCHAR(65535)\"},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "        Outputs=[\n",
    "            {\n",
    "                \"Name\": \"AVG_STAR_RATING_SQL_STREAM\",\n",
    "                \"LambdaOutput\": {\n",
    "                    \"ResourceARN\": \"{}\".format(lambda_fn_arn_cloudwatch),\n",
    "                    \"RoleARN\": \"{}\".format(iam_role_lambda_arn),\n",
    "                },\n",
    "                \"DestinationSchema\": {\"RecordFormatType\": \"CSV\"},\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"ANOMALY_SCORE_SQL_STREAM\",\n",
    "                \"LambdaOutput\": {\n",
    "                    \"ResourceARN\": \"{}\".format(lambda_fn_arn_sns),\n",
    "                    \"RoleARN\": \"{}\".format(iam_role_kinesis_arn),\n",
    "                },\n",
    "                \"DestinationSchema\": {\"RecordFormatType\": \"CSV\"},\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"APPROXIMATE_COUNT_SQL_STREAM\",\n",
    "                \"KinesisStreamsOutput\": {\n",
    "                    \"ResourceARN\": \"{}\".format(stream_arn),\n",
    "                    \"RoleARN\": \"{}\".format(iam_role_kinesis_arn),\n",
    "                },\n",
    "                \"DestinationSchema\": {\"RecordFormatType\": \"CSV\"},\n",
    "            },\n",
    "        ],\n",
    "        ApplicationCode=sql_code,\n",
    "    )\n",
    "    print(\"SQL application {} successfully created.\".format(kinesis_data_analytics_app_name))\n",
    "    print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceInUseException\":\n",
    "        print(\"SQL App {} already exists.\".format(kinesis_data_analytics_app_name))\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "response = kinesis_analytics.describe_application(ApplicationName=kinesis_data_analytics_app_name)\n",
    "print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "\n",
    "input_id = response[\"ApplicationDetail\"][\"InputDescriptions\"][0][\"InputId\"]\n",
    "print(input_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Kinesis Data Analytics App\n",
    "\n",
    "try:\n",
    "    response = kinesis_analytics.start_application(\n",
    "        ApplicationName=kinesis_data_analytics_app_name,\n",
    "        InputConfigurations=[{\"Id\": input_id, \"InputStartingPositionConfiguration\": {\"InputStartingPosition\": \"NOW\"}}],\n",
    "    )\n",
    "    print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"ResourceInUseException\":\n",
    "        print(\"Application {} is already starting.\".format(kinesis_data_analytics_app_name))\n",
    "    else:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "response = kinesis_analytics.describe_application(ApplicationName=kinesis_data_analytics_app_name)\n",
    "\n",
    "app_status = response[\"ApplicationDetail\"][\"ApplicationStatus\"]\n",
    "print(\"Application status {}\".format(app_status))\n",
    "\n",
    "while app_status != \"RUNNING\":\n",
    "    time.sleep(5)\n",
    "    response = kinesis_analytics.describe_application(ApplicationName=kinesis_data_analytics_app_name)\n",
    "    app_status = response[\"ApplicationDetail\"][\"ApplicationStatus\"]\n",
    "    print(\"Application status {}\".format(app_status))\n",
    "\n",
    "print(\"Application status {}\".format(app_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Customer Reviews On Kinesis Data Firehose\n",
    "\n",
    "<img src=\"img/kinesis-complete.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firehoses = firehose.list_delivery_streams(DeliveryStreamType=\"DirectPut\")\n",
    "print(json.dumps(firehoses, indent=4, sort_keys=True, default=str))\n",
    "\n",
    "# Download Dataset\n",
    "!aws s3 cp 's3://dsoaws/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz' ./data/\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\",\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "print(df.shape)\n",
    "\n",
    "df_star_rating_and_review_body = df[[\"review_id\", \"star_rating\", \"product_category\", \"review_body\"]][0:1]\n",
    "df_star_rating_and_review_body.to_csv(sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Producer Application Writing Records to the Stream\n",
    "\n",
    "# Open Lambda (CloudWatch) Logs\n",
    "# Open Custom CloudWatch Metrics\n",
    "# Open Kinesis Data Analytics Console UI\n",
    "\n",
    "# Put Records onto Firehose\n",
    "firehose_response = firehose.describe_delivery_stream(DeliveryStreamName=firehose_name)\n",
    "print(json.dumps(firehose_response, indent=4, sort_keys=True, default=str))\n",
    "%%time\n",
    "\n",
    "step = 1\n",
    "\n",
    "for start_idx in range(0, 500, step):\n",
    "    end_idx = start_idx + step\n",
    "\n",
    "    df_star_rating_and_review_body = df[[\"review_id\", \"product_category\", \"review_body\"]][start_idx:end_idx]\n",
    "\n",
    "    reviews_tsv = df_star_rating_and_review_body.to_csv(sep=\"\\t\", header=None, index=False)\n",
    "\n",
    "    # print(reviews_tsv.encode('utf-8'))\n",
    "\n",
    "    response = firehose.put_record(Record={\"Data\": reviews_tsv.encode(\"utf-8\")}, DeliveryStreamName=firehose_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review S3 Kinesis Source Records\n",
    "\n",
    "\n",
    "They should look like this: \n",
    "    \n",
    "```\n",
    "R2EI7QLPK4LF7U\tDigital_Software\tSo far so good\n",
    "R1W5OMFK1Q3I3O\tDigital_Software\tNeeds a little more work.....\n",
    "RPZWSYWRP92GI\t Digital_Software\tPlease cancel.\n",
    "R2WQWM04XHD9US\tDigital_Software\tWorks as Expected!\n",
    "```\n",
    "\n",
    "# Review S3 Kinesis Transformed Records\n",
    "\n",
    "They should look like this:\n",
    "    \n",
    "```\n",
    "R2EI7QLPK4LF7U\t5\tDigital_Software\tSo far so good\n",
    "R1W5OMFK1Q3I3O\t3\tDigital_Software\tNeeds a little more work.....\n",
    "RPZWSYWRP92GI\t 1\tDigital_Software\tPlease cancel.\n",
    "R2WQWM04XHD9US\t5\tDigital_Software\tWorks as Expected!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinesis Analytics UI: \n",
    "\n",
    "<img src=\"../readme_pics/kinesis_analytics_1.png\" width=\"80%\" align=\"left\">\n",
    "<img src=\"../readme_pics/kinesis_analytics_5.png\" width=\"80%\" align=\"left\">\n",
    "<img src=\"../readme_pics/kinesis_analytics_4.png\" width=\"80%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Put Anomaly Data Onto Stream\n",
    "Here, we are hard-coding a bad review to trigger an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "anomaly_step = 1\n",
    "\n",
    "for start_idx in range(0, 10000, anomaly_step):\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    df_anomalies = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"review_id\": str(timestamp),\n",
    "                \"product_category\": \"Digital_Software\",\n",
    "                \"review_body\": \"This is an awful waste of time.\",\n",
    "            },\n",
    "        ],\n",
    "        columns=[\"review_id\", \"star_rating\", \"product_category\", \"review_body\"],\n",
    "    )\n",
    "\n",
    "    reviews_tsv_anomalies = df_anomalies.to_csv(sep=\"\\t\", header=None, index=False)\n",
    "\n",
    "    response = firehose.put_record(\n",
    "        Record={\"Data\": reviews_tsv_anomalies.encode(\"utf-8\")}, DeliveryStreamName=firehose_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../readme_pics/kinesis_analytics_3.png\" width=\"80%\" align=\"left\">"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
