{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing two different TF models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B testing, also known as split testing, is a method used to compare two versions of a product or system to determine which one performs better. In the context of machine learning models deployed on a SageMaker endpoint with production variants, A/B testing involves directing a portion of the incoming traffic to different variants of the deployed models to assess their performance under real-world conditions.\n",
    "\n",
    "Here's how A/B testing is implemented within the framework of SageMaker production variants:\n",
    "\n",
    "Create Production Variants: Each version of the machine learning model is represented as a production variant. These variants could differ based on factors like hardware (CPU/GPU), data subsets (e.g., comedy/drama movies), or deployment regions (e.g., US West or Germany North).\n",
    "\n",
    "Specify Traffic Allocation: Allocate a certain percentage of incoming traffic to each production variant. For instance, you might decide to split the traffic evenly between two variants (50%/50%) for a straightforward A/B test.\n",
    "\n",
    "Deploy and Monitor: Deploy the production variants within a SageMaker endpoint. As traffic flows into the endpoint, requests are distributed according to the specified allocation. Monitor the performance metrics, such as accuracy, latency, or other relevant KPIs, for each variant.\n",
    "\n",
    "Analyze Results: Compare the performance of the different variants based on the collected metrics. This analysis helps determine which variant is more effective in meeting the desired objectives, such as improved accuracy or reduced latency.\n",
    "\n",
    "Adjust and Iterate: Depending on the results, you can adjust the traffic allocation, make modifications to the models, or even introduce entirely new variants. A/B testing is an iterative process that allows for continuous improvement.\n",
    "\n",
    "Decision Making: Based on the findings, you can make informed decisions about whether to choose one variant over another for full deployment or to introduce further enhancements.\n",
    "By leveraging A/B testing in the context of SageMaker production variants, organizations can systematically evaluate and compare different versions of machine learning models, ensuring that decisions are data-driven and align with business objectives. This approach mitigates risks associated with deploying untested models and provides a mechanism for ongoing optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](../data/readme_pics/AB-Testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used traffic splitting to direct subsets of users to different model variants for the purpose of comparing and testing different models in live production. The goal is to see which variants perform better. Often, these tests need to run for a long period of time (weeks) to be statistically significant. The figure shows 2 different recommendation models deployed using a random 50-50 traffic split between the 2 variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "cw = boto3.Session().client(service_name=\"cloudwatch\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete prev SageMaker Endpoint\n",
    "%store -r autopilot_endpoint_name\n",
    "sm.delete_endpoint(EndpointName=autopilot_endpoint_name)\n",
    "print(\"Autopilot Endpoint has been deleted to save resources.\")\n",
    "%store -r training_job_name\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the Model to the Notebook\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz\n",
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/\n",
    "# Show the Prediction Signature\n",
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant A Model\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "print(inference_image_uri)\n",
    "\n",
    "timestamp = \"{}\".format(int(time.time()))\n",
    "\n",
    "model_a_name = \"{}-{}-{}\".format(training_job_name, \"varianta\", timestamp)\n",
    "\n",
    "sess.create_model_from_job(\n",
    "    name=model_a_name, training_job_name=training_job_name, role=role, image_uri=inference_image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant B Model\n",
    "model_b_name = \"{}-{}-{}\".format(training_job_name, \"variantb\", timestamp)\n",
    "\n",
    "sess.create_model_from_job(\n",
    "    name=model_b_name, training_job_name=training_job_name, role=role, image_uri=inference_image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canary Rollouts and A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canary rollouts are a deployment strategy commonly employed to introduce new machine learning models into production environments cautiously. This method involves releasing the new model to only a small subset of users, typically around 5%, allowing for live testing in a production setting without affecting the entire user base immediately. The rationale behind canary rollouts is to minimize potential negative impacts by exposing the new model to a limited audience before widespread adoption.\n",
    "\n",
    "To implement canary rollouts with a service like Amazon SageMaker, instead of a straightforward deploy() operation, an approach involving Endpoint Configuration with multiple variants is utilized. In the context of SageMaker, an \"Endpoint Configuration\" is a collection of settings, including the type and number of instances, that determines how an inference endpoint is configured.\n",
    "\n",
    "Here's how canary rollouts is achieved:\n",
    "\n",
    "Create Endpoint Configuration: Define an endpoint configuration with multiple variants, each corresponding to a different version of your machine learning model. For a canary rollout, this would include the existing model version (baseline) and the new version (canary).\n",
    "\n",
    "Specify Traffic Distribution:Assign the desired traffic distribution for each variant. In the case of a canary rollout, you might allocate 95% of the traffic to the existing model (baseline) and 5% to the new model (canary).\n",
    "\n",
    "Update Endpoint: Apply the new endpoint configuration to your existing inference endpoint. This step triggers SageMaker to gradually shift the traffic according to the specified distribution.\n",
    "\n",
    "Monitor and Evaluate: Monitor the performance and behavior of the canary model in the production environment. This includes tracking key metrics, such as accuracy and latency, to ensure that the new model meets expectations.\n",
    "\n",
    "Gradual Rollout or Rollback: Depending on the observed performance, you can choose to gradually increase the traffic to the canary model or rollback to the baseline model if issues arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "timestamp = \"{}\".format(int(time.time()))\n",
    "\n",
    "endpoint_config_name = \"{}-{}-{}\".format(training_job_name, \"abtest\", timestamp)\n",
    "\n",
    "variantA = production_variant(\n",
    "    model_name=model_a_name,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"VariantA\",\n",
    "    initial_weight=50,\n",
    ")\n",
    "\n",
    "variantB = production_variant(\n",
    "    model_name=model_b_name,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"VariantB\",\n",
    "    initial_weight=50,\n",
    ")\n",
    "\n",
    "endpoint_config = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name, ProductionVariants=[variantA, variantB]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ab_endpoint_name = \"{}-{}-{}\".format(training_job_name, \"abtest\", timestamp)\n",
    "\n",
    "endpoint_response = sm.create_endpoint(EndpointName=model_ab_endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "%store model_ab_endpoint_name\n",
    "%store -r experiment_name\n",
    "%store -r trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.trial import Trial\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "timestamp = \"{}\".format(int(time.time()))\n",
    "trial = Trial.load(trial_name=trial_name)\n",
    "print(trial)\n",
    "tracker_deploy = Tracker.create(display_name=\"deploy\", sagemaker_boto_client=sm)\n",
    "deploy_trial_component_name = tracker_deploy.trial_component.trial_component_name\n",
    "print(\"Deploy trial component name {}\".format(deploy_trial_component_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the `deploy` Trial Component and Tracker as a Component to the Trial\n",
    "trial.add_trial_component(tracker_deploy.trial_component)\n",
    "# Track the Endpoint Name\n",
    "tracker_deploy.log_parameters(\n",
    "    {\n",
    "        \"endpoint_name\": model_ab_endpoint_name,\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "tracker_deploy.trial_component.save()\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment_name,\n",
    "    metric_names=[\"validation:accuracy\"],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "\n",
    "lineage_df = lineage_table.dataframe()\n",
    "lineage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a Prediction from an Application\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "    endpoint_name=model_ab_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    model_name=\"saved_model\",\n",
    "    model_version=0,\n",
    "    content_type=\"application/jsonlines\",\n",
    "    accept_type=\"application/jsonlines\",\n",
    "    serializer=JSONLinesSerializer(),\n",
    "    deserializer=JSONLinesDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the `star_rating` with Ad Hoc `review_body` Samples\n",
    "inputs = [{\"features\": [\"This is great!\"]}, {\"features\": [\"This is bad.\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    print(\"Predicted star_rating: {}\".format(predicted_class))\n",
    "\n",
    "\n",
    "# Predict the `star_rating` with `review_body` Samples from our TSV's\n",
    "df_reviews = pd.read_csv(\n",
    "    \"./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\",\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "df_sample_reviews = df_reviews[[\"review_body\", \"star_rating\"]].sample(n=50)\n",
    "df_sample_reviews = df_sample_reviews.reset_index()\n",
    "print(df_sample_reviews.shape)\n",
    "\n",
    "\n",
    "def predict(review_body):\n",
    "    inputs = [{\"features\": [review_body]}]\n",
    "    predicted_classes = predictor.predict(inputs)\n",
    "    return predicted_classes[0][\"predicted_label\"]\n",
    "\n",
    "\n",
    "df_sample_reviews[\"predicted_class\"] = df_sample_reviews[\"review_body\"].map(predict)\n",
    "df_sample_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the REST Endpoint Performance Metrics in CloudWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Amazon SageMaker emits metrics such as Latency and Invocations for each variant in Amazon CloudWatch. I have queried CloudWatch to get the InvocationsPerVariant to show how invocations are split across variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the REST Endpoint Performance Metrics in a Dataframe\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_invocation_metrics_for_endpoint_variant(\n",
    "    endpoint_name, namespace_name, metric_name, variant_name, start_time, end_time\n",
    "):\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=namespace_name,\n",
    "        MetricName=metric_name,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60,\n",
    "        Statistics=[\"Sum\"],\n",
    "        Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name}, {\"Name\": \"VariantName\", \"Value\": variant_name}],\n",
    "    )\n",
    "\n",
    "    if metrics[\"Datapoints\"]:\n",
    "        return (\n",
    "            pd.DataFrame(metrics[\"Datapoints\"])\n",
    "            .sort_values(\"Timestamp\")\n",
    "            .set_index(\"Timestamp\")\n",
    "            .drop(\"Unit\", axis=1)\n",
    "            .rename(columns={\"Sum\": variant_name})\n",
    "        )\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def plot_endpoint_metrics_for_variants(endpoint_name, namespace_name, metric_name, start_time=None):\n",
    "    try:\n",
    "        start_time = start_time or datetime.now() - timedelta(minutes=60)\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        metrics_variantA = get_invocation_metrics_for_endpoint_variant(\n",
    "            endpoint_name=model_ab_endpoint_name,\n",
    "            namespace_name=namespace_name,\n",
    "            metric_name=metric_name,\n",
    "            variant_name=variantA[\"VariantName\"],\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "        )\n",
    "\n",
    "        metrics_variantB = get_invocation_metrics_for_endpoint_variant(\n",
    "            endpoint_name=model_ab_endpoint_name,\n",
    "            namespace_name=namespace_name,\n",
    "            metric_name=metric_name,\n",
    "            variant_name=variantB[\"VariantName\"],\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "        )\n",
    "\n",
    "        metrics_variants = metrics_variantA.join(metrics_variantB, how=\"outer\")\n",
    "        metrics_variants.plot()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing the Metrics for Each Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(20)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"/aws/sagemaker/Endpoints\", metric_name=\"CPUUtilization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"Invocations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"InvocationsPerInstance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"ModelLatency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting All Traffic to Variant B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_endpoint_config = [\n",
    "    {\n",
    "        \"VariantName\": variantA[\"VariantName\"],\n",
    "        \"DesiredWeight\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"VariantName\": variantB[\"VariantName\"],\n",
    "        \"DesiredWeight\": 100,\n",
    "    },\n",
    "]\n",
    "sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=model_ab_endpoint_name, DesiredWeightsAndCapacities=updated_endpoint_config\n",
    ")\n",
    "\n",
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Some Predictions\n",
    "df_sample_reviews[\"predicted_class\"] = df_sample_reviews[\"review_body\"].map(predict)\n",
    "df_sample_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Each Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(20)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"/aws/sagemaker/Endpoints\", metric_name=\"CPUUtilization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"Invocations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"InvocationsPerInstance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"ModelLatency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Variant A to Reduce Cost\n",
    "# Modifing the Endpoint Configuration to only use variant B.\n",
    "import time\n",
    "\n",
    "timestamp = \"{}\".format(int(time.time()))\n",
    "\n",
    "updated_endpoint_config_name = \"{}-{}\".format(training_job_name, timestamp)\n",
    "\n",
    "updated_endpoint_config = sm.create_endpoint_config(\n",
    "    EndpointConfigName=updated_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variantB[\"VariantName\"],\n",
    "            \"ModelName\": model_b_name,  # Only specify variant B to remove variant A\n",
    "            \"InstanceType\": \"ml.m5.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 100,\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.update_endpoint(EndpointName=model_ab_endpoint_name, EndpointConfigName=updated_endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Some Predictions\n",
    "df_sample_reviews[\"predicted_class\"] = df_sample_reviews[\"review_body\"].map(predict)\n",
    "df_sample_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Metrics for Each Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(20)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"/aws/sagemaker/Endpoints\", metric_name=\"CPUUtilization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"Invocations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"InvocationsPerInstance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "time.sleep(5)\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, namespace_name=\"AWS/SageMaker\", metric_name=\"ModelLatency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=model_ab_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
