{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting Data Into The Cloud\n",
    "\n",
    "In this section, we will describe a typical scenario in which an application writes data into an Amazon S3 Data Lake and the data needs to be accessed by both the data science / machine learning team, as well as the business intelligence / data analyst team as shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../data/readme_pics/data-ingestion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a **data scientist or machine learning engineer**, you want to have access to all of the raw data, and be able to quickly explore it. We will show you how to leverage **Amazon Athena** as an interactive query service to analyze data in Amazon S3 using standard SQL, without moving the data. \n",
    "* In the first step, we will register the TSV data in our S3 bucket with Athena, and then run some ad-hoc queries on the dataset. \n",
    "* We will also show how you can easily convert the TSV data into the more query-optimized, columnar file format Apache Parquet. \n",
    "\n",
    "Your **business intelligence team and data analysts** might also want to have a subset of the data in a data warehouse which they can then transform, and query with their standard SQL clients to create reports and visualize trends. We will show you how to leverage **Amazon Redshift**, a fully managed data warehouse service, to \n",
    "\n",
    "* insert TSV data into Amazon Redshift, but also be able to combine the data warehouse queries with the data that’s still in our S3 data lake via **Amazon Redshift Spectrum**. \n",
    "* You can also use Amazon Redshift’s data lake export functionality to unload data back into our S3 data lake in Parquet file format. \n",
    "\n",
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/dsoaws/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip list\n",
    "!pip install --disable-pip-version-check -q pip --upgrade > /dev/null\n",
    "!pip install --disable-pip-version-check -q wrapt --upgrade > /dev/null\n",
    "!pip install --disable-pip-version-check -q awscli==1.18.216 boto3==1.16.56 botocore==1.19.56\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.29.0\n",
    "!pip install --disable-pip-version-check -q smdebug==1.0.1\n",
    "!pip install --disable-pip-version-check -q sagemaker-experiments==0.1.26\n",
    "!conda install -y pytorch==1.6.0 -c pytorch\n",
    "!pip install --disable-pip-version-check -q tensorflow==2.3.1\n",
    "!pip install --disable-pip-version-check -q transformers==3.5.1\n",
    "!pip install --disable-pip-version-check -q PyAthena==2.1.0\n",
    "!pip install --disable-pip-version-check -q SQLAlchemy==1.3.22\n",
    "!pip install --disable-pip-version-check -q psycopg2-binary==2.9.1\n",
    "!pip install --disable-pip-version-check -q stepfunctions==2.0.0rc1\n",
    "!conda install -y zip\n",
    "!pip install --disable-pip-version-check -q matplotlib==3.1.3\n",
    "!pip install --disable-pip-version-check -q seaborn==0.10.0\n",
    "setup_dependencies_passed = True\n",
    "%store setup_dependencies_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)\n",
    "print(\"Default bucket: {}\".format(bucket))\n",
    "setup_s3_bucket_passed = True\n",
    "%store setup_s3_bucket_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAM Roles and Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "config = Config(retries={\"max_attempts\": 10, \"mode\": \"adaptive\"})\n",
    "iam = boto3.client(\"iam\", config=config)\n",
    "# Get SageMaker Execution Role Name\n",
    "role_name = role.split(\"/\")[-1]\n",
    "print(\"Role name: {}\".format(role_name))\n",
    "setup_iam_roles_passed = False\n",
    "\n",
    "admin = False\n",
    "\n",
    "post_policies = iam.list_attached_role_policies(RoleName=role_name)[\"AttachedPolicies\"]\n",
    "for post_policy in post_policies:\n",
    "    if post_policy[\"PolicyName\"] == \"AdministratorAccess\":\n",
    "        admin = True\n",
    "        setup_iam_roles_passed = True\n",
    "        print(\"All set\")\n",
    "        break\n",
    "\n",
    "if not admin:   \n",
    "        print(\"*************** [ERROR] *****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not admin:\n",
    "    pre_policies = iam.list_attached_role_policies(RoleName=role_name)[\"AttachedPolicies\"]\n",
    "\n",
    "    required_policies = [\"IAMFullAccess\"]\n",
    "\n",
    "    for pre_policy in pre_policies:\n",
    "        for role_req in required_policies:\n",
    "            if pre_policy[\"PolicyName\"] == role_req:\n",
    "                print(\"Attached: {}\".format(pre_policy[\"PolicyName\"]))\n",
    "                try:\n",
    "                    required_policies.remove(pre_policy[\"PolicyName\"])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    if len(required_policies) > 0:\n",
    "        print(\n",
    "            \"*************** [ERROR] You need to attach the following policies in order to continue with this workshop *****************\\n\"\n",
    "        )\n",
    "        for required_policy in required_policies:\n",
    "            print(\"Not Attached: {}\".format(required_policy))\n",
    "    else:\n",
    "        print(\"[OK] You are all set to continue with this notebook!\")\n",
    "else:\n",
    "    print(\"[OK] You are all set to continue with this notebook!\")\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    policy = \"AdministratorAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonSageMakerFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"IAMFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonS3FullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"ComprehendFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonAthenaFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"SecretsManagerReadWrite\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonRedshiftFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonEC2ContainerRegistryFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AWSStepFunctionsFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonKinesisFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonKinesisFirehoseFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    policy = \"AmazonKinesisAnalyticsFullAccess\"\n",
    "    response = iam.attach_role_policy(PolicyArn=\"arn:aws:iam::aws:policy/{}\".format(policy), RoleName=role_name)\n",
    "    print(\"Policy {} has been succesfully attached to role: {}\".format(policy, role_name))\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":\n",
    "        print(\"[OK] Policy is already attached.\")\n",
    "    elif e.response[\"Error\"][\"Code\"] == \"LimitExceeded\":\n",
    "        print(\"[OK]\")\n",
    "    else:\n",
    "        print(\"*************** [ERROR] {} *****************\".format(e))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# role = iam.get_role(RoleName=role_name)\n",
    "post_policies = iam.list_attached_role_policies(RoleName=role_name)[\"AttachedPolicies\"]\n",
    "\n",
    "required_policies = [\n",
    "    \"AdministratorAccess\",\n",
    "    \"SecretsManagerReadWrite\",\n",
    "    \"IAMFullAccess\",\n",
    "    \"AmazonS3FullAccess\",\n",
    "    \"AmazonAthenaFullAccess\",\n",
    "    \"ComprehendFullAccess\",\n",
    "    \"AmazonEC2ContainerRegistryFullAccess\",\n",
    "    \"AmazonRedshiftFullAccess\",\n",
    "    \"AWSStepFunctionsFullAccess\",\n",
    "    \"AmazonSageMakerFullAccess\",\n",
    "    \"AmazonKinesisFullAccess\",\n",
    "    \"AmazonKinesisFirehoseFullAccess\",\n",
    "    \"AmazonKinesisAnalyticsFullAccess\",\n",
    "]\n",
    "\n",
    "admin = False\n",
    "\n",
    "for post_policy in post_policies:\n",
    "    if post_policy[\"PolicyName\"] == \"AdministratorAccess\":\n",
    "        admin = True\n",
    "        try:\n",
    "            required_policies.remove(post_policy[\"PolicyName\"])\n",
    "        except:\n",
    "            break\n",
    "    else:\n",
    "        try:\n",
    "            required_policies.remove(post_policy[\"PolicyName\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if not admin and len(required_policies) > 0:\n",
    "    print(\"*************** [ERROR]*****************\")\n",
    "    for required_policy in required_policies:\n",
    "        print(\"Not Attached: {}\".format(required_policy))\n",
    "else:\n",
    "    setup_iam_roles_passed = True\n",
    "    print(\"All Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy TSV Data To S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://dsoaws/amazon-reviews-pds/tsv/ \n",
    "!aws s3 ls s3://dsoaws/amazon-reviews-pds/parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "# Set S3 Source Location (Public S3 Bucket)\n",
    "s3_public_path_tsv = \"s3://dsoaws/amazon-reviews-pds/tsv\"\n",
    "%store s3_public_path_tsv\n",
    "# Set S3 Destination Location (Our Private S3 Bucket)\n",
    "s3_private_path_tsv = \"s3://{}/amazon-reviews-pds/tsv\".format(bucket)\n",
    "print(s3_private_path_tsv)\n",
    "%store s3_private_path_tsv\n",
    "#  Copy Data From the Public S3 Bucket to our Private S3 Bucket in this Account\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Software_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Gift_Card_v1_00.tsv.gz\"\n",
    "\n",
    "!aws s3 ls $s3_private_path_tsv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Athena Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_db_passed = False\n",
    "%store -r s3_public_path_tsv\n",
    "%store -r s3_private_path_tsv\n",
    "\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "database_name = \"dsoaws\"\n",
    "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
    "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)\n",
    "statement = \"CREATE DATABASE IF NOT EXISTS {}\".format(database_name)\n",
    "print(statement)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "statement = \"SHOW DATABASES\"\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)\n",
    "if database_name in df_show.values:\n",
    "    ingest_create_athena_db_passed = True\n",
    "%store ingest_create_athena_db_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register TSV Data With Athena\n",
    "This will create an Athena table in the Glue Catalog (Hive Metastore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_table_tsv_passed = False\n",
    "%store -r ingest_create_athena_db_passed\n",
    "table_name_tsv = \"amazon_reviews_tsv\"\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {}.{}(\n",
    "         marketplace string,\n",
    "         customer_id string,\n",
    "         review_id string,\n",
    "         product_id string,\n",
    "         product_parent string,\n",
    "         product_title string,\n",
    "         product_category string,\n",
    "         star_rating int,\n",
    "         helpful_votes int,\n",
    "         total_votes int,\n",
    "         vine string,\n",
    "         verified_purchase string,\n",
    "         review_headline string,\n",
    "         review_body string,\n",
    "         review_date string\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\\\t' LINES TERMINATED BY '\\\\n' LOCATION '{}'\n",
    "TBLPROPERTIES ('compressionType'='gzip', 'skip.header.line.count'='1')\"\"\".format(\n",
    "    database_name, table_name_tsv, s3_private_path_tsv\n",
    ")\n",
    "\n",
    "print(statement)\n",
    "pd.read_sql(statement, conn)\n",
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)\n",
    "if table_name_tsv in df_show.values:\n",
    "    ingest_create_athena_table_tsv_passed = True\n",
    "%store ingest_create_athena_table_tsv_passed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert TSV Data To Parquet with Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_create_athena_table_parquet_passed = False\n",
    "%store -r ingest_create_athena_table_tsv_passed\n",
    "table_name_parquet = \"amazon_reviews_parquet\"\n",
    "# SQL statement to execute\n",
    "statement = \"\"\"CREATE TABLE IF NOT EXISTS {}.{}\n",
    "WITH (format = 'PARQUET', external_location = '{}', partitioned_by = ARRAY['product_category']) AS\n",
    "SELECT marketplace,\n",
    "         customer_id,\n",
    "         review_id,\n",
    "         product_id,\n",
    "         product_parent,\n",
    "         product_title,\n",
    "         star_rating,\n",
    "         helpful_votes,\n",
    "         total_votes,\n",
    "         vine,\n",
    "         verified_purchase,\n",
    "         review_headline,\n",
    "         review_body,\n",
    "         CAST(YEAR(DATE(review_date)) AS INTEGER) AS year,\n",
    "         DATE(review_date) AS review_date,\n",
    "         product_category\n",
    "FROM {}.{}\"\"\".format(\n",
    "    database_name, table_name_parquet, s3_path_parquet, database_name, table_name_tsv\n",
    ")\n",
    "\n",
    "print(statement)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "# Load partitions by running `MSCK REPAIR TABLE`\n",
    "statement = \"MSCK REPAIR TABLE {}.{}\".format(database_name, table_name_parquet)\n",
    "print(statement)\n",
    "pd.read_sql(statement, conn)\n",
    "\n",
    "# Show Partitions\n",
    "statement = \"SHOW PARTITIONS {}.{}\".format(database_name, table_name_parquet)\n",
    "print(statement)\n",
    "pd.read_sql(statement, conn)\n",
    "df_partitions.head(5)\n",
    "\n",
    "statement = \"SHOW TABLES in {}\".format(database_name)\n",
    "df_show = pd.read_sql(statement, conn)\n",
    "df_show.head(5)\n",
    "\n",
    "if table_name_parquet in df_show.values:\n",
    "    ingest_create_athena_table_parquet_passed = True\n",
    "\n",
    "%store ingest_create_athena_table_parquet_passed\n",
    "\n",
    "# Sample Query\n",
    "product_category = \"Digital_Software\"\n",
    "\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    WHERE product_category = '{}' LIMIT 100\"\"\".format(\n",
    "    database_name, table_name_parquet, product_category\n",
    ")\n",
    "\n",
    "print(statement)\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Amazon Redshift Cluster\n",
    "\n",
    "Amazon Redshift is a fully managed data warehouse which allows you to run complex analytic queries against petabytes of structured data. Your queries are distributed and parallelized across multiple physical resources, and you can easily scale your Amazon Redshift environment up and down depending on your business needs. \n",
    "\n",
    "_Note:  This notebook requires that you are running this SageMaker Notebook Instance in a VPC with access to the Redshift cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Lake S3 vs Data Warehouse Redshift**\n",
    "\n",
    "One of the fundamental differences between data lakes and data warehouses is that while you ingest and store huge amounts of raw, unprocessed data in your data lake, you normally only load some fraction of your recent data into your data warehouse. Depending on your business and analytics use case, this might be data from the past couple of months, a year, or maybe the past 2 years. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup IAM Access To Read From S3 and Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 10,\n",
    "      'mode': 'adaptive'\n",
    "   }\n",
    ")\n",
    "iam = boto3.client('iam', config=config)\n",
    "sts = boto3.client('sts')\n",
    "redshift = boto3.client('redshift')\n",
    "sm = boto3.client('sagemaker')\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "# Create AssumeRolePolicyDocument\n",
    "assume_role_policy_doc = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"redshift.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "} \n",
    "\n",
    "iam_redshift_role_name = 'DSOAWS_Redshift'\n",
    "try:\n",
    "    iam_role_redshift = iam.create_role(\n",
    "        RoleName=iam_redshift_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "        Description='DSOAWS Redshift Role'\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Role already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Get the Role ARN\n",
    "role = iam.get_role(RoleName='DSOAWS_Redshift')\n",
    "iam_role_redshift_arn = role['Role']['Arn']\n",
    "print(iam_role_redshift_arn)\n",
    "\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "print(account_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Self-Managed Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_redshift_to_s3 = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "my_redshift_to_athena = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"athena:*\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:CreateDatabase\",\n",
    "                \"glue:DeleteDatabase\",\n",
    "                \"glue:GetDatabase\",\n",
    "                \"glue:GetDatabases\",\n",
    "                \"glue:UpdateDatabase\",\n",
    "                \"glue:CreateTable\",\n",
    "                \"glue:DeleteTable\",\n",
    "                \"glue:BatchDeleteTable\",\n",
    "                \"glue:UpdateTable\",\n",
    "                \"glue:GetTable\",\n",
    "                \"glue:GetTables\",\n",
    "                \"glue:BatchCreatePartition\",\n",
    "                \"glue:CreatePartition\",\n",
    "                \"glue:DeletePartition\",\n",
    "                \"glue:BatchDeletePartition\",\n",
    "                \"glue:UpdatePartition\",\n",
    "                \"glue:GetPartition\",\n",
    "                \"glue:GetPartitions\",\n",
    "                \"glue:BatchGetPartition\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:ListBucketMultipartUploads\",\n",
    "                \"s3:ListMultipartUploadParts\",\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:CreateBucket\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::aws-athena-query-results-*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::athena-examples*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:ListAllMyBuckets\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sns:ListTopics\",\n",
    "                \"sns:GetTopicAttributes\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"cloudwatch:PutMetricAlarm\",\n",
    "                \"cloudwatch:DescribeAlarms\",\n",
    "                \"cloudwatch:DeleteAlarms\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lakeformation:GetDataAccess\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "my_redshift_to_sagemaker = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"sagemaker:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "my_redshift_to_sagemaker_passrole = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": f'arn:aws:iam::{account_id}:role/*'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create Policy Objects\n",
    "try:\n",
    "    policy_redshift_s3 = iam.create_policy(\n",
    "      PolicyName='DSOAWS_RedshiftPolicyToS3',\n",
    "      PolicyDocument=json.dumps(my_redshift_to_s3)\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Get ARN\n",
    "policy_redshift_s3_arn = f'arn:aws:iam::{account_id}:policy/DSOAWS_RedshiftPolicyToS3'\n",
    "print(policy_redshift_s3_arn)\n",
    "\n",
    "try:\n",
    "    policy_redshift_athena = iam.create_policy(\n",
    "      PolicyName='DSOAWS_RedshiftPolicyToAthena',\n",
    "      PolicyDocument=json.dumps(my_redshift_to_athena)\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Get ARN\n",
    "policy_redshift_athena_arn = f'arn:aws:iam::{account_id}:policy/DSOAWS_RedshiftPolicyToAthena'\n",
    "print(policy_redshift_athena_arn)\n",
    "\n",
    "try:\n",
    "    policy_redshift_sagemaker = iam.create_policy(\n",
    "      PolicyName='DSOAWS_RedshiftPolicyToSageMaker',\n",
    "      PolicyDocument=json.dumps(my_redshift_to_sagemaker)\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "# Get ARN\n",
    "policy_redshift_sagemaker_arn = f'arn:aws:iam::{account_id}:policy/DSOAWS_RedshiftPolicyToSageMaker'\n",
    "print(policy_redshift_sagemaker_arn)\n",
    "\n",
    "try:\n",
    "    policy_redshift_sagemaker_passrole = iam.create_policy(\n",
    "      PolicyName='DSOAWS_RedshiftPolicyToSageMakerPassRole',\n",
    "      PolicyDocument=json.dumps(my_redshift_to_sagemaker_passrole)\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy already exists\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Get ARN\n",
    "policy_redshift_sagemaker_passrole_arn = f'arn:aws:iam::{account_id}:policy/DSOAWS_RedshiftPolicyToSageMakerPassRole'\n",
    "print(policy_redshift_sagemaker_passrole_arn)\n",
    "\n",
    "# Attach Policies To Role\n",
    "\n",
    "# Attach DSOAWS_RedshiftPolicyToAthena policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_athena_arn,\n",
    "        RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "# Attach DSOAWS_RedshiftPolicyToS3 policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_s3_arn,\n",
    "        RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "        \n",
    "# Attach DSOAWS_RedshiftPolicyToSageMaker policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_sagemaker_arn,\n",
    "        RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Attach DSOAWS_RedshiftPolicyToSageMakerPassRole policy\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=policy_redshift_sagemaker_passrole_arn,\n",
    "        RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "\n",
    "# Update Trust relationshiops to include both Redshift and SageMaker\n",
    "my_redshift_to_sagemaker_assumerole = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"redshift.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"sagemaker.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}  \n",
    "try:\n",
    "    response = iam.update_assume_role_policy(\n",
    "        PolicyDocument=json.dumps(my_redshift_to_sagemaker_assumerole),\n",
    "        RoleName=iam_redshift_role_name\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Security Group ID \n",
    "\n",
    "* Make sure the Redshift VPC is the same this notebook is running within\n",
    "* Make sure the VPC has the following 2 properties enabled\n",
    " *     DNS resolution = Enabled\n",
    " *     DNS hostnames = Enabled\n",
    "* This allows private, internal access to Redshift from this SageMaker notebook using the fully qualified endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    domain_id = sm.list_domains()['Domains'][0]['DomainId']\n",
    "    describe_domain_response = sm.describe_domain(DomainId=domain_id)\n",
    "    vpc_id = describe_domain_response['VpcId']\n",
    "    security_groups = ec2.describe_security_groups()['SecurityGroups']\n",
    "    for security_group in security_groups:\n",
    "        if vpc_id == security_group['VpcId']:\n",
    "            security_group_id = security_group['GroupId']\n",
    "    print(security_group_id)    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    notebook_instance_name = sm.list_notebook_instances()['NotebookInstances'][0]['NotebookInstanceName']\n",
    "    notebook_instance = sm.describe_notebook_instance(NotebookInstanceName=notebook_instance_name)\n",
    "    security_group_id = notebook_instance['SecurityGroups'][0]\n",
    "    print(security_group_id)    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Secret in Secrets Manager\n",
    "\n",
    "AWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secretsmanager = boto3.client('secretsmanager')\n",
    "\n",
    "try:\n",
    "    response = secretsmanager.create_secret(\n",
    "        Name='dsoaws_redshift_login',\n",
    "        Description='DSOAWS Redshift Login',\n",
    "        SecretString='[{\"username\":\"dsoaws\"},{\"password\":\"Password9\"}]',\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'name',\n",
    "                'Value': 'dsoaws_redshift_login'\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceExistsException':\n",
    "        print(\"Secret already exists. This is ok.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "secret = secretsmanager.get_secret_value(SecretId='dsoaws_redshift_login')\n",
    "cred = json.loads(secret['SecretString'])\n",
    "\n",
    "master_user_name = cred[0]['username']\n",
    "master_user_pw = cred[1]['password']\n",
    "\n",
    "# Redshift configuration parameters\n",
    "redshift_cluster_identifier = 'dsoaws'\n",
    "database_name = 'dsoaws'\n",
    "cluster_type = 'multi-node'\n",
    "\n",
    "# Note that only some Instance Types support Redshift Query Editor \n",
    "# (https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor.html)\n",
    "node_type = 'dc2.large'\n",
    "number_nodes = '2' \n",
    "\n",
    "response = redshift.create_cluster(\n",
    "        DBName=database_name,\n",
    "        ClusterIdentifier=redshift_cluster_identifier,\n",
    "        ClusterType=cluster_type,\n",
    "        NodeType=node_type,\n",
    "        NumberOfNodes=int(number_nodes),       \n",
    "        MasterUsername=master_user_name,\n",
    "        MasterUserPassword=master_user_pw,\n",
    "        IamRoles=[iam_role_redshift_arn],\n",
    "        VpcSecurityGroupIds=[security_group_id],\n",
    "        Port=5439,\n",
    "        PubliclyAccessible=False\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "import time\n",
    "\n",
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "print(cluster_status)\n",
    "\n",
    "while cluster_status != 'available':\n",
    "    time.sleep(10)\n",
    "    response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "    print(cluster_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TSV Data From S3/Athena into Redshift\n",
    "\n",
    "We can leverage our previously created table in Amazon Athena with its metadata and schema information stored in the AWS Glue Data Catalog to access our data in S3 through Redshift Spectrum. All we need to do is create an external schema in Redshift, point it to our AWS Glue Data Catalog, and point Redshift to the database we’ve created.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = secretsmanager.get_secret_value(SecretId='dsoaws_redshift_login')\n",
    "cred = json.loads(secret['SecretString'])\n",
    "\n",
    "master_user_name = cred[0]['username']\n",
    "master_user_pw = cred[1]['password']\n",
    "redshift_cluster_identifier = 'dsoaws'\n",
    "\n",
    "database_name_redshift = 'dsoaws'\n",
    "database_name_athena = 'dsoaws'\n",
    "\n",
    "redshift_port = '5439'\n",
    "\n",
    "schema_redshift = 'redshift'\n",
    "schema_athena = 'athena'\n",
    "\n",
    "table_name_tsv = 'amazon_reviews_tsv'\n",
    "import time\n",
    "\n",
    "response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "print(cluster_status)\n",
    "\n",
    "while cluster_status != 'available':\n",
    "    time.sleep(10)\n",
    "    response = redshift.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "    print(cluster_status)\n",
    "\n",
    "# Get Redshift Endpoint Address & IAM Role\n",
    "redshift_endpoint_address = response['Clusters'][0]['Endpoint']['Address']\n",
    "iam_role = response['Clusters'][0]['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "print('Redshift endpoint: {}'.format(redshift_endpoint_address))\n",
    "print('IAM Role: {}'.format(iam_role))\n",
    "\n",
    "# Create Redshift Connection\n",
    "import awswrangler as wr\n",
    "\n",
    "con_redshift = wr.data_api.redshift.connect(\n",
    "    cluster_id=redshift_cluster_identifier,\n",
    "    database=database_name_redshift,\n",
    "    db_user=master_user_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redshift Spectrum\n",
    "Amazon Redshift Spectrum directly queries data in S3, using the same SQL syntax of Amazon Redshift. You can also run queries that span both the frequently accessed data stored locally in Amazon Redshift and your full datasets stored cost-effectively in S3.\n",
    "\n",
    "To use Redshift Spectrum, your cluster needs authorization to access data catalog in Amazon Athena and your data files in Amazon S3. You provide that authorization by referencing an AWS Identity and Access Management (IAM) role that is attached to your cluster. \n",
    "\n",
    "To use this capability in from your Amazon SageMaker notebook:\n",
    "\n",
    "* Register your Athena database `dsoaws` with Redshift Spectrum\n",
    "* Query Your Data in Amazon S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "CREATE EXTERNAL SCHEMA IF NOT EXISTS {} FROM DATA CATALOG \n",
    "    DATABASE '{}' \n",
    "    IAM_ROLE '{}'\n",
    "    REGION '{}'\n",
    "    CREATE EXTERNAL DATABASE IF NOT EXISTS\n",
    "\"\"\".format(schema_athena, database_name_athena, iam_role, region_name)\n",
    "\n",
    "print(statement)\n",
    "wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "statement = \"\"\"\n",
    "SELECT product_category, COUNT(star_rating) AS count_star_rating\n",
    "    FROM {}.{}\n",
    "    GROUP BY product_category\n",
    "    ORDER BY count_star_rating DESC\n",
    "\"\"\".format(schema_athena, table_name_tsv)\n",
    "\n",
    "print(statement)\n",
    "\n",
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TSV Data Into Redshift\n",
    "\n",
    "Create Redshift tables with Customer Reviews data for each year we wish to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `redshift` Schema\n",
    "statement = \"\"\"CREATE SCHEMA IF NOT EXISTS {}\"\"\".format(schema_redshift)\n",
    "\n",
    "wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Redshift Tables for Each Year We Wish to Load\n",
    "When you create a table, you can specify one or more columns as the **sort key**. Amazon Redshift stores your data on disk in sorted order according to the sort key. This means, you can optimize your table by choosing a sort key that reflects your most frequently used query types. If you query a lot of recent data, you can specify a timestamp column as the sort key. If you frequently query based on range or equality filtering on one column, you should choose that column as the sort key. \n",
    "\n",
    "As we are going to run a lot of queries in the next chapter filtering on `product_category`, let’s choose that one as our sort key. \n",
    "\n",
    "You can also define a distribution style for every table. When you load data into a table, Redshift distributes the rows of the table among your cluster nodes according to the table’s distribution style. When you run a query, the query optimizer redistributes the rows to the cluster nodes as needed to perform any joins and aggregations. So our goal should be to optimize the rows distribution to minimize needed data movements. There are three distribution styles from which you can choose from: \n",
    "\n",
    "* KEY distribution - distribute the rows according to the values in one column\n",
    "* ALL distribution - distribute a copy of the entire table to every node\n",
    "* EVEN distribution - the rows are distributed across all nodes in a round-robin-fashion which is the default distribution style\n",
    "\n",
    "For our table, we’ve chosen **KEY distribution** based on `product_id` as this column has a high cardinality, shows an even distribution and can be used to join with other tables. \n",
    "\n",
    "Now we are ready to copy the data from S3 into our new Redshift table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table function, pass session, table name prefix and start & end year\n",
    "\n",
    "def create_redshift_table_tsv(wr, con_redshift, table_name_prefix, start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1, 1):\n",
    "        current_table_name = table_name_prefix+'_'+str(year)\n",
    "        statement = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS redshift.{}( \n",
    "             marketplace varchar(2),\n",
    "             customer_id varchar(8),\n",
    "             review_id varchar(14),\n",
    "             product_id varchar(10) DISTKEY,\n",
    "             product_parent varchar(9),\n",
    "             product_title varchar(400),\n",
    "             product_category varchar(24),\n",
    "             star_rating int,\n",
    "             helpful_votes int,\n",
    "             total_votes int,\n",
    "             vine varchar(1),\n",
    "             verified_purchase varchar(1),\n",
    "             review_headline varchar(128),\n",
    "             review_body varchar(65535),\n",
    "             review_date varchar(10),\n",
    "             year int)  SORTKEY (product_category)\n",
    "        \"\"\".format(current_table_name)\n",
    "\n",
    "        wr.data_api.redshift.read_sql_query(\n",
    "            sql=statement,\n",
    "            con=con_redshift,\n",
    "        )\n",
    "    print(\"Done.\")\n",
    "create_redshift_table_tsv(wr, con_redshift, 'amazon_reviews_tsv', 2014, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert TSV Data into New Redshift Tables\n",
    "\n",
    "For such bulk inserts, you can either use a `COPY` command, or an `INSERT INTO` command. In general, the `COPY` command is preferred, as it loads data in parallel and more efficiently from Amazon S3, or other supported data sources. \n",
    "\n",
    "If you are loading data or a subset of data from one table into another, you can use the `INSERT INTO` command with a `SELECT` clause for high-performance data insertion. As we’re loading our data from the `athena.amazon_reviews_tsv` table, let’s choose this option. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT INTO function, pass session, table name prefix and start & end year\n",
    "\n",
    "def insert_into_redshift_table_tsv(wr, con_redshift, table_name_prefix, start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1, 1):\n",
    "        print(year)\n",
    "        current_table_name = table_name_prefix+'_'+str(year)\n",
    "        statement = \"\"\"\n",
    "            INSERT \n",
    "            INTO\n",
    "                redshift.{}\n",
    "                SELECT\n",
    "                    marketplace,\n",
    "                    customer_id,\n",
    "                    review_id,\n",
    "                    product_id,\n",
    "                    product_parent,\n",
    "                    product_title,\n",
    "                    product_category,\n",
    "                    star_rating,\n",
    "                    helpful_votes,\n",
    "                    total_votes,\n",
    "                    vine,\n",
    "                    verified_purchase,\n",
    "                    review_headline,\n",
    "                    review_body,\n",
    "                    review_date,\n",
    "                    CAST(DATE_PART_YEAR(TO_DATE(review_date, 'YYYY-MM-DD')) AS INTEGER) AS year\n",
    "                FROM\n",
    "                    athena.amazon_reviews_tsv             \n",
    "                WHERE\n",
    "                    year = {}\n",
    "            \"\"\".format(current_table_name, year)\n",
    "\n",
    "        wr.data_api.redshift.read_sql_query(\n",
    "            sql=statement,\n",
    "            con=con_redshift,\n",
    "        )\n",
    "\n",
    "        df.head()\n",
    "    print(\"Done.\")\n",
    "    insert_into_redshift_table_tsv(wr, con_redshift, 'amazon_reviews_tsv', 2014, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Both Athena And Redshift With `Redshift Spectrum`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Query Across Both Redshift and Athena in a single query\n",
    "# Use `UNION ALL` across 2 Redshift tables (2015, 2014) and the rest from Athena/S3 (2013-1995)\n",
    "\n",
    "statement = \"\"\"\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2015\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2015.product_category, year\n",
    "UNION ALL\n",
    "SELECT year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM redshift.amazon_reviews_tsv_2014\n",
    "  GROUP BY redshift.amazon_reviews_tsv_2014.product_category, year\n",
    "UNION ALL\n",
    "SELECT CAST(DATE_PART_YEAR(TO_DATE(review_date, 'YYYY-MM-DD')) AS INTEGER) AS year, product_category, COUNT(star_rating) AS count_star_rating\n",
    "  FROM athena.amazon_reviews_tsv\n",
    "  WHERE year <= 2013\n",
    "  GROUP BY athena.amazon_reviews_tsv.product_category, year\n",
    "ORDER BY product_category ASC, year DESC\n",
    "\"\"\"\n",
    "\n",
    "print(statement)\n",
    "df = wr.data_api.redshift.read_sql_query(\n",
    "    sql=statement,\n",
    "    con=con_redshift,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Athena vs. Redshift?\n",
    "\n",
    "### Amazon Athena\n",
    "Athena should be your preferred choice when running ad-hoc SQL queries on data that is stored in Amazon S3. It doesn’t require you to set up or manage any infrastructure resources, and you don’t need to move any data. It supports structured, unstructured, and semi-structured data. With Athena, you are defining a **“schema on read”** - you basically just log in, create a table and you are good to go. \n",
    "\n",
    "### Amazon Redshift\n",
    "Redshift is targeted for modern data analytics on large sets of structured data. Here, you need to have a predefined **“schema on write”**. Unlike serverless Athena, Redshift requires you to create a cluster (compute and storage resources), ingest the data and build tables before you can start to query, but caters to performance and scale. So for any highly-relational data with a transactional nature (data gets updated), workloads which involve complex joins, and latency requirements to be sub-second, Redshift is the right choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Redshift Data Lake Export\n",
    "\n",
    "Redshift Data Lake Export gives you the ability to unload the result of a Redshift query to your S3 data lake in Apache Parquet format. This enables you to save data transformation and enrichment you have done in Redshift into your S3 data lake in an open format.\n",
    "\n",
    "You can specify one or more partition columns so that unloaded data is automatically partitioned into folders in your Amazon S3 bucket. \n",
    "\n",
    "For example, you can choose to unload our customer reviews data and partition it by `product_category`. This enables your queries to take advantage of partition pruning and skip scanning non-relevant partitions, improving query performance and minimizing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_redshift_table(wr, con_redshift, table_name_prefix, start_year, end_year, s3_path, iam_role):\n",
    "    for year in range(start_year, end_year+1, 1):\n",
    "        current_table_name = table_name_prefix+'_'+str(year)\n",
    "        statement = \"\"\"\n",
    "            UNLOAD ('SELECT marketplace, customer_id, review_id, product_id, product_parent, \n",
    "                product_title, product_category, star_rating, helpful_votes, total_votes, \n",
    "                vine, verified_purchase, review_headline, review_body, review_date, year \n",
    "            FROM redshift.{}')\n",
    "            TO '{}/{}/'\n",
    "            IAM_ROLE '{}'\n",
    "            PARQUET PARALLEL ON \n",
    "            PARTITION BY (product_category)\n",
    "        \"\"\".format(current_table_name, s3_path, year, iam_role)\n",
    "\n",
    "        wr.data_api.redshift.read_sql_query(\n",
    "            sql=statement,\n",
    "            con=con_redshift,\n",
    "        )\n",
    "    print(\"Done.\")\n",
    "\n",
    "unload_redshift_table(wr, con_redshift, 'amazon_reviews_tsv', 2014, 2015, s3_path_parquet_unload, iam_role)\n",
    "print(s3_path_parquet_unload)\n",
    "!aws s3 ls $s3_path_parquet_unload/2014/\n",
    "!aws s3 ls $s3_path_parquet_unload/2015/"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
